<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>（翻译）传统和深度学习模型在文本分类中的应用综述与基准 | qubeijun的博客</title><meta name="author" content="qubeijun"><meta name="copyright" content="qubeijun"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="（翻译）传统和深度学习模型在文本分类中的应用综述与基准">
<meta property="og:type" content="article">
<meta property="og:title" content="（翻译）传统和深度学习模型在文本分类中的应用综述与基准">
<meta property="og:url" content="https://qubeijun.github.io/posts/11019.html">
<meta property="og:site_name" content="qubeijun的博客">
<meta property="og:description" content="（翻译）传统和深度学习模型在文本分类中的应用综述与基准">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qubeijun.github.io/image/avatar.jpg">
<meta property="article:published_time" content="2019-04-15T09:01:46.000Z">
<meta property="article:modified_time" content="2024-10-08T07:04:06.120Z">
<meta property="article:author" content="qubeijun">
<meta property="article:tag" content="cnn">
<meta property="article:tag" content="分类">
<meta property="article:tag" content="rnn">
<meta property="article:tag" content="GloVe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qubeijun.github.io/image/avatar.jpg"><link rel="shortcut icon" href="/image/avatar.jpg"><link rel="canonical" href="https://qubeijun.github.io/posts/11019.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f4fed142810e41206e562eb165b433b7";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '（翻译）传统和深度学习模型在文本分类中的应用综述与基准',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-08 15:04:06'
}</script><link rel="stylesheet" href="https://unpkg.com/element-ui/lib/theme-chalk/index.css"><script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script><script src="https://unpkg.com/element-ui/lib/index.js"></script><link rel="stylesheet" href="/html/100days/colorcycle.css"><link rel="stylesheet" href="/html/100days/christmaslights.css"><link rel="stylesheet" href="/html/100days/border.css"><script src="/html/100days/colorcycle.js"></script><script src="/html/100days/christmaslights.js"></script><script src="/html/100days/border.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/image/background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">78</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/galleryGroup/"><i class="fa-fw fa-solid fa-image"></i><span> 图库</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 后院</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/dairy/"><i class="fa-fw fa-solid fa-book"></i><span> 个人日记</span></a></li><li><a class="site-page child" href="/father/"><i class="fa-fw fa-solid fa-backward"></i><span> 父亲往事</span></a></li><li><a class="site-page child" href="/son/"><i class="fa-fw fa-solid fa-envelope"></i><span> 儿子的信</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-heart"></i><span> 爱情</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/html/love1/love1.html"><i class="fa-fw fas fa-heart"></i><span> 爱情1</span></a></li><li><a class="site-page child" href="/html/love2/love2.html"><i class="fa-fw fas fa-heart"></i><span> 爱情2</span></a></li><li><a class="site-page child" href="/html/love3/love3.html"><i class="fa-fw fas fa-heart"></i><span> 爱情3</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-fw fas fa-comment-dots"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa-solid fa-train-subway"></i><span> 开往</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/image/avatar.jpg" alt="Logo"><span class="site-name">qubeijun的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">（翻译）传统和深度学习模型在文本分类中的应用综述与基准</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/galleryGroup/"><i class="fa-fw fa-solid fa-image"></i><span> 图库</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 后院</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/dairy/"><i class="fa-fw fa-solid fa-book"></i><span> 个人日记</span></a></li><li><a class="site-page child" href="/father/"><i class="fa-fw fa-solid fa-backward"></i><span> 父亲往事</span></a></li><li><a class="site-page child" href="/son/"><i class="fa-fw fa-solid fa-envelope"></i><span> 儿子的信</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-heart"></i><span> 爱情</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/html/love1/love1.html"><i class="fa-fw fas fa-heart"></i><span> 爱情1</span></a></li><li><a class="site-page child" href="/html/love2/love2.html"><i class="fa-fw fas fa-heart"></i><span> 爱情2</span></a></li><li><a class="site-page child" href="/html/love3/love3.html"><i class="fa-fw fas fa-heart"></i><span> 爱情3</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-fw fas fa-comment-dots"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa-solid fa-train-subway"></i><span> 开往</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">（翻译）传统和深度学习模型在文本分类中的应用综述与基准</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-04-15T09:01:46.000Z" title="发表于 2019-04-15 17:01:46">2019-04-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-08T07:04:06.120Z" title="更新于 2024-10-08 15:04:06">2024-10-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span></div></div></div><article class="post-content" id="article-container"><span id="more"></span>
<p>原文：<a target="_blank" rel="noopener" href="https://ahmedbesbes.com/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification.html">Overview and benchmark of traditional and deep learning models in text classification</a></p>
<p>本文是我在试验Twitter数据情感分析时所写的前一篇文章的扩展。 回到过去，我探索了一个简单的模型：一个在keras上训练的双层前馈神经网络。 输入推文被表示为文档向量，这是由组成推文的单词的嵌入的加权平均值产生的。</p>
<p>我使用的嵌入是一个word2vec模型，我使用gensim从头开始训练语料库。 任务是二进制分类，我能够使用此设置达到79％的准确率。</p>
<p>这篇文章的目标是探索在同一数据集上训练的其他NLP模型，然后在给定的测试集上对它们各自的性能进行基准测试。</p>
<p>我们将通过不同的模型：从依赖于词汇表示的简单模型到部署卷积/循环网络的重型机器：我们将看到我们的准确度是否超过79％！</p>
<p>我将从简单的模型开始，逐步增加复杂性。 目标也在于表明简单模型也能很好地工作。</p>
<p>所以我要试试这些：</p>
<ul><li>用词ngram的逻辑回归</li>
	<li>具有字符ngram的逻辑回归</li>
	<li>用词和字符ngram的逻辑回归</li>
	<li>没有预先训练的嵌入的递归神经网络（双向GRU）</li>
	<li>具有GloVe预训练嵌入的递归神经网络（双向GRU）</li>
	<li>多通道卷积神经网络</li>
	<li>RNN（双向GRU）+ CNN模型</li>
</ul><p>到本文结束时，您将获得每种NLP技术的样板代码。 它将帮助您启动您的NLP项目并最终获得最先进的结果（其中一些模型非常强大）。</p>
<p>我们还将提供一个全面的基准，我们将从中了解哪种模型最适合预测推文的情感。</p>
<p>在相关的git repo中，我将发布不同的模型，它们的预测以及测试集。 您可以自己尝试并对结果充满信心</p>
<p>Let's get started!</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">import warnings</span><br><span class="line">warnings.simplefilter(&quot;ignore&quot;, UserWarning)</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">pd.options.mode.chained_assignment = None</span><br><span class="line">import numpy as np </span><br><span class="line">from string import punctuation</span><br><span class="line"></span><br><span class="line">from nltk.tokenize import word_tokenize</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.metrics import accuracy_score, auc, roc_auc_score</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line"></span><br><span class="line">import scipy</span><br><span class="line">from scipy.sparse import hstack</span><br></pre></td></tr></table></figure>
<h1 id="0---Data-pre-processing">0 - Data pre-processing</h1>
<p>可以从<a target="_blank" rel="noopener" href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/">此链接</a>下载数据集。</p>
<p>我们将加载它并将自己限制在我们需要的变量（Sentiment和SentimentText）。</p>
<p>它包含1578614个分类推文，每行标记为1表示积极情绪，0表示负面情绪。</p>
<p>作者建议使用1/10来测试算法，其余用于训练。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(&#x27;./data/tweets.csv&#x27;, encoding=&#x27;latin1&#x27;, usecols=[&#x27;Sentiment&#x27;, &#x27;SentimentText&#x27;])</span><br><span class="line">data.columns = [&#x27;sentiment&#x27;, &#x27;text&#x27;]</span><br><span class="line">data = data.sample(frac=1, random_state=42)</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1578614, 2)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for row in data.head(10).iterrows():</span><br><span class="line">    print(row[ 1][&#x27;sentiment&#x27;], row[ 1][&#x27;text&#x27;])</span><br></pre></td></tr></table></figure>
<p>1 http://www.popsugar.com/2999655 keep voting for robert pattinson in the popsugar100 as well!! <br />
1 @GamrothTaylor I am starting to worry about you, only I have Navy Seal type sleep hours. <br />
0 sunburned...no sunbaked!    ow.  it hurts to sit.<br />
1 Celebrating my 50th birthday by doing exactly the same as I do every other day - working on our websites.  It's just another day.   <br />
1 Leah and Aiden Gosselin are the cutest kids on the face of the Earth <br />
1 @MissHell23 Oh. I didn't even notice.  <br />
0 WTF is wrong with me?!!! I'm completely miserable. I need to snap out of this <br />
0 Was having the best time in the gym until I got to the car and had messages waiting for me... back to the down stage! <br />
1 @JENTSYY oh what happened?? <br />
0 @catawu Ghod forbid he should feel responsible for anything! </p>
<p>推文有很多噪声，让我们通过删除网址，主题标签和用户提及来清理它们。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def tokenize(tweet):</span><br><span class="line">    tweet = re.sub(r&#x27;http\S+&#x27;, &#x27;&#x27;, tweet)</span><br><span class="line">    tweet = re.sub(r&quot;#(\w+)&quot;, &#x27;&#x27;, tweet)</span><br><span class="line">    tweet = re.sub(r&quot;@(\w+)&quot;, &#x27;&#x27;, tweet)</span><br><span class="line">    tweet = re.sub(r&#x27;[^\w\s]&#x27;, &#x27;&#x27;, tweet)</span><br><span class="line">    tweet = tweet.strip().lower()</span><br><span class="line">    tokens = word_tokenize(tweet)</span><br><span class="line">    return tokens</span><br></pre></td></tr></table></figure>
<p>数据清理完毕后，我们将其保存在磁盘上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data[&#x27;tokens&#x27;] = data.text.progress_map(tokenize)</span><br><span class="line">data[&#x27;cleaned_text&#x27;] = data[&#x27;tokens&#x27;].map(lambda tokens: &#x27; &#x27;.join(tokens))</span><br><span class="line">data[[&#x27;sentiment&#x27;, &#x27;cleaned_text&#x27;]].to_csv(&#x27;./data/cleaned_text.csv&#x27;)</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(&#x27;./data/cleaned_text.csv&#x27;)</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1575026, 2)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td> </td>
			<td>sentiment</td>
			<td>cleaned_text</td>
		</tr><tr><td>0</td>
			<td>0</td>
			<td>playing with my routers looks like i might hav...</td>
		</tr><tr><td>1</td>
			<td>1</td>
			<td>sleeeep agh im so tired and they wrote gay on ...</td>
		</tr><tr><td>2</td>
			<td>0</td>
			<td>alan ignored me during the concert boo</td>
		</tr><tr><td>3</td>
			<td>1</td>
			<td>really want some mini eggs why are they only a...</td>
		</tr><tr><td>4</td>
			<td>0</td>
			<td>thanks guys sorry i had to miss your show at m...</td>
		</tr></tbody></table><p>现在清理了数据集，让我们准备一个训练/测试分割来构建我们的模型。</p>
<p>我们将在整个笔记本中使用这种分割。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_test, y_train, y_test = train_test_split(data[&#x27;cleaned_text&#x27;], </span><br><span class="line">                                                    data[&#x27;sentiment&#x27;], </span><br><span class="line">                                                    test_size=0.1, </span><br><span class="line">                                                    random_state=42,</span><br><span class="line">                                                    stratify=data[&#x27;sentiment&#x27;])</span><br><span class="line"></span><br><span class="line">print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1417523,) (157503,) (1417523,) (157503,)</span><br></pre></td></tr></table></figure>
<p>我将测试标签保存在磁盘上供以后使用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(y_test).to_csv(&#x27;./predictions/y_true.csv&#x27;, index=False, encoding=&#x27;utf-8&#x27;)</span><br></pre></td></tr></table></figure>
<p>让我们现在开始应用一些机器学习：</p>
<h1 id="1---Bag-of-word-model-based-on-word-ngrams">1 - Bag of word model based on word ngrams</h1>
<p>所以。 什么是n-gram？</p>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/ngrams.png" /></p>
<p> 正如我们在此图中看到的那样，n-gram只是源文本中可以找到的长度为n的相邻单词（在本例中）的所有组合。</p>
<p>在我们的模型中，我们将使用unigrams（n = 1）和bigrams（n = 2）作为特征。</p>
<p> 因此，数据集将表示为矩阵，其中每行对应一条推文，每列对应从文本中提取的特征（unigram或bigram）（在标记化和清理之后）。 每个单元格将是tf-idf分数。 （我们也可以使用简单的计数但是tf-idf通常更常用，通常效果更好）。 我们将此矩阵称为文档术语矩阵。</p>
<p>你可以想象，150万个推文语料库中独特的unigrams和bigrams的数量是巨大的。 实际上，出于计算原因，我们将此数字设置为固定值。 您可以使用交叉验证来确定此值。</p>
<p>这是矢量化后语料库应该是什么样子。</p>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/tfidf.jpg" /></p>
<p><strong>I like pizza a lot</strong></p>
<p>假设我们想使用上述特征将此句子提供给预测模型。</p>
<p>鉴于我们正在使用unigrams和bigrams，该模型将提取以下功能：</p>
<p><strong>i, like, pizza, a, lot, i like, like pizza, pizza a, a lot</strong></p>
<p>因此，句子将由包含大量零的大小为N（=令牌总数）的向量和这些ngram的tf-idf得分组成。 所以你可以清楚地看到我们将处理大而稀疏的向量。</p>
<p>在处理大型和稀疏数据时，线性模型通常表现良好。 此外，它们比其他类型的模型（例如基于树的模型）更快地训练。</p>
<p>从过去的经验我可以看出，逻辑回归在稀疏tf idf矩阵之上运行良好。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vectorizer_word = TfidfVectorizer(max_features=40000,</span><br><span class="line">                             min_df=5, </span><br><span class="line">                             max_df=0.5, </span><br><span class="line">                             analyzer=&#x27;word&#x27;, </span><br><span class="line">                             stop_words=&#x27;english&#x27;, </span><br><span class="line">                             ngram_range=(1, 2))</span><br><span class="line"></span><br><span class="line">vectorizer_word.fit(x_train, leave=False)</span><br><span class="line"></span><br><span class="line">tfidf_matrix_word_train = vectorizer_word.transform(x_train)</span><br><span class="line">tfidf_matrix_word_test = vectorizer_word.transform(x_test)</span><br></pre></td></tr></table></figure>
<p>在为训练集和测试集生成tfidf矩阵之后，我们可以构建我们的第一个模型进行测试。</p>
<p>tifidf矩阵是逻辑回归的特征。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr_word = LogisticRegression(solver=&#x27;sag&#x27;, verbose=2)</span><br><span class="line">lr_word.fit(tfidf_matrix_word_train, y_train)</span><br></pre></td></tr></table></figure>
<p>一旦模型被训练，我们将其应用于测试数据以获得预测。 然后我们将这些值以及模型保存在磁盘上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">joblib.dump(lr_word, &#x27;./models/lr_word_ngram.pkl&#x27;)</span><br><span class="line"></span><br><span class="line">y_pred_word = lr_word.predict(tfidf_matrix_word_test)</span><br><span class="line">pd.DataFrame(y_pred_word, columns=[&#x27;y_pred&#x27;]).to_csv(&#x27;./predictions/lr_word_ngram.csv&#x27;, index=False)</span><br></pre></td></tr></table></figure>
<p>让我们看看我们得到的准确度分数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred_word = pd.read_csv(&#x27;./predictions/lr_word_ngram.csv&#x27;)</span><br><span class="line">print(accuracy_score(y_test, y_pred_word))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.782042246814</span><br></pre></td></tr></table></figure>
<p>第一个模型的准确度为78.2％！ 还不错。 让我们转到下一个模型。</p>
<h1 id="2---Bag-of-word-model-based-on-character-ngrams">2 - Bag of word model based on character ngrams</h1>
<p>我们从未说过ngram只是用于单词。 我们也可以在角色级别应用它们。</p>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/ngrams_char.jpg" /></p>
<p>你看到它来了，对吗？ 我们将把相同的代码应用于字符ngram，而我们将达到4-grams。</p>
<p>这基本上意味着像“我喜欢这部电影”这样的句子将具有以下特征：</p>
<p><strong>I, l, i, k, e, …, I li, lik, like, …, this, … , is m, s mo, movi, …</strong></p>
<p>字符ngram令人惊讶地非常有效。 在建模语言任务时，它们甚至可以胜过单词标记。 例如，垃圾邮件过滤器或本地语言识别严重依赖于字符ngram。</p>
<p>与之前学习单词组合的模型不同，该模型学习字母组合，可以处理单词的形态构成。</p>
<p>基于字符的表示的一个优点是更好地处理拼写错误的单词。</p>
<p>让我们运行相同的管道：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vectorizer_char = TfidfVectorizer(max_features=40000,</span><br><span class="line">                             min_df=5, </span><br><span class="line">                             max_df=0.5, </span><br><span class="line">                             analyzer=&#x27;char&#x27;, </span><br><span class="line">                             ngram_range=(1, 4))</span><br><span class="line"></span><br><span class="line">vectorizer_char.fit(tqdm_notebook(x_train, leave=False));</span><br><span class="line"></span><br><span class="line">tfidf_matrix_char_train = vectorizer_char.transform(x_train)</span><br><span class="line">tfidf_matrix_char_test = vectorizer_char.transform(x_test)</span><br><span class="line"></span><br><span class="line">lr_char = LogisticRegression(solver=&#x27;sag&#x27;, verbose=2)</span><br><span class="line">lr_char.fit(tfidf_matrix_char_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_char = lr_char.predict(tfidf_matrix_char_test)</span><br><span class="line">joblib.dump(lr_char, &#x27;./models/lr_char_ngram.pkl&#x27;)</span><br><span class="line"></span><br><span class="line">pd.DataFrame(y_pred_char, columns=[&#x27;y_pred&#x27;]).to_csv(&#x27;./predictions/lr_char_ngram.csv&#x27;, index=False)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred_char = pd.read_csv(&#x27;./predictions/lr_char_ngram.csv&#x27;)</span><br><span class="line">print(accuracy_score(y_test, y_pred_char))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.80420055491</span><br></pre></td></tr></table></figure>
<p>精度80.4％！ 字符图表比word-ngrams表现更好。</p>
<h1 id="3---Bag-of-word-model-based-on-word-and-character-ngrams">3 - Bag of word model based on word and character ngrams</h1>
<p>字符ngram特征似乎提供比字ngram更好的准确性。 但是两者的结合怎么样：单词+字符ngrams？</p>
<p>让我们连接我们生成的两个tfidf矩阵并构建一个新的混合tfidf矩阵。</p>
<p>这个模型将帮助我们学习一个单词及其可能的邻居的身份以及它的形态结构。</p>
<p>这些属性结合在一起。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))</span><br><span class="line">tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))</span><br><span class="line"></span><br><span class="line">lr_word_char = LogisticRegression(solver=&#x27;sag&#x27;, verbose=2)</span><br><span class="line">lr_word_char.fit(tfidf_matrix_word_char_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_word_char = lr_word_char.predict(tfidf_matrix_word_char_test)</span><br><span class="line">joblib.dump(lr_word_char, &#x27;./models/lr_word_char_ngram.pkl&#x27;)</span><br><span class="line"></span><br><span class="line">pd.DataFrame(y_pred_word_char, columns=[&#x27;y_pred&#x27;]).to_csv(&#x27;./predictions/lr_word_char_ngram.csv&#x27;, index=False)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred_word_char = pd.read_csv(&#x27;./predictions/lr_word_char_ngram.csv&#x27;)</span><br><span class="line">print(accuracy_score(y_test, y_pred_word_char))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.81423845895</span><br></pre></td></tr></table></figure>
<p>太棒了：81.4％的准确率。 我们只增加了一整个单位，并且超过了之前的两个设置。</p>
<p>在我们继续之前，我们可以对词袋模型说些什么呢？</p>
<ul><li>优点：由于它们的简单性，它们可以令人惊讶地强大，它们训练速度快，易于理解。</li>
	<li>缺点：尽管ngrams在单词之间带来了一些上下文，但是单词模型包在模拟序列中单词之间的长期依赖性时失败。</li>
</ul><p>现在我们将深入研究深度学习模型。 深度学习优于词袋模型的原因是能够捕捉句子中单词之间的顺序依赖性。 由于发明了称为回归神经网络的特殊神经网络架构，这是可能的。</p>
<p>我不会介绍RNN的理论基础，但这里有一个值得一读的<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">链接</a>。 它来自Cristopher Olah的博客。 它详细介绍了LSTM：长期短期记忆。 一种特殊的RNN。</p>
<p>在开始之前，我们必须设置一个深度学习专用环境，在Tensorflow之上使用Keras。 老实说，我试图在我的个人笔记本电脑上运行所有东西，但考虑到数据集的重要大小和RNN架构的复杂性，这是不切实际的。 完全没有。</p>
<p>一个很好的选择是AWS。 我通常在EC2 p2.xlarge实例上使用这种深度学习AMI。 Amazon AMI是预先配置的VM映像，其中安装了所有软件包（Tensorflow，PyTocrh，Keras等）。 我强烈推荐这个我已经使用了一段时间。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from keras.preprocessing.text import Tokenizer</span><br><span class="line">from keras.preprocessing.text import text_to_word_sequence</span><br><span class="line">from keras.preprocessing.sequence import pad_sequences</span><br><span class="line"></span><br><span class="line">from keras.models import Model</span><br><span class="line">from keras.models import Sequential</span><br><span class="line"></span><br><span class="line">from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D</span><br><span class="line">from keras.layers import Reshape, Flatten, Dropout, Concatenate</span><br><span class="line">from keras.layers import SpatialDropout1D, concatenate</span><br><span class="line">from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D</span><br><span class="line"></span><br><span class="line">from keras.callbacks import Callback</span><br><span class="line">from keras.optimizers import Adam</span><br><span class="line"></span><br><span class="line">from keras.callbacks import ModelCheckpoint, EarlyStopping</span><br><span class="line">from keras.models import load_model</span><br><span class="line">from keras.utils.vis_utils import plot_model</span><br></pre></td></tr></table></figure>
<h1 id="4---Recurrent-Neural-Network-without-pre-trained-embedding">4 - Recurrent Neural Network without pre-trained embedding</h1>
<p>RNN可能看起来很吓人。 虽然它们很难理解，但它们非常有趣。 它们封装了一个非常漂亮的设计，克服了传统神经网络在处理序列数据时出现的缺点：文本，时间序列，视频，DNA序列等。</p>
<p>RNN是一系列神经网络块，它们像链一样彼此链接。 每个人都将消息传递给继任者。</p>
<p>再次，如果你想深入了解内部机制，我强烈推荐Colah的<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">博客</a>，其中包含下图。 </p>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/rnn_unrolled.png" /></p>
<p>我们将处理文本数据，这是一种序列类型。 单词的顺序对表示非常重要。 希望RNN能够处理这个问题并捕获长期依赖关系。</p>
<p>要在文本数据上使用Keras，我们必须对其进行预处理。 为此，我们可以使用Keras的Tokenizer类。 该对象采用num_words参数作为参数，这是基于字频率进行标记化后保留的最大字数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MAX_NB_WORDS = 80000</span><br><span class="line">tokenizer = Tokenizer(num_words=MAX_NB_WORDS)</span><br><span class="line"></span><br><span class="line">tokenizer.fit_on_texts(data[&#x27;cleaned_text&#x27;])</span><br></pre></td></tr></table></figure>
<p>一旦将标记化器安装在数据上，我们就可以使用它将文本字符串转换为数字序列。</p>
<p>这些数字代表字典中每个单词的位置（将其视为映射）。</p>
<p>我们来看一个例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train[15]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;breakfast time happy time&#x27;</span><br></pre></td></tr></table></figure>
<p>这是令牌器将其转换为数字序列的方式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.texts_to_sequences([x_train[15]])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[530, 50, 119, 50]]</span><br></pre></td></tr></table></figure>
<p>现在让我们在训练和测试序列上应用这个标记器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_sequences = tokenizer.texts_to_sequences(x_train)</span><br><span class="line">test_sequences = tokenizer.texts_to_sequences(x_test)</span><br></pre></td></tr></table></figure>
<p>现在推文被映射到整数列表。 但是，由于它们具有不同的长度，我们仍然不能将它们堆叠在一起。 希望Keras允许将序列填充为0到最大长度。 我们将此长度设置为35.（这是推文中令牌的最大数量）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MAX_LENGTH = 35</span><br><span class="line">padded_train_sequences = pad_sequences( train_sequences, maxlen=MAX_LENGTH)</span><br><span class="line">padded_test_sequences = pad_sequences( test_sequences, maxlen=MAX_LENGTH)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">padded_train_sequences</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[    0,     0,     0, ...,  2383,   284,     9],</span><br><span class="line">       [    0,     0,     0, ...,    13,    30,    76],</span><br><span class="line">       [    0,     0,     0, ...,    19,    37, 45231],</span><br><span class="line">       ..., </span><br><span class="line">       [    0,     0,     0, ...,    43,   502,  1653],</span><br><span class="line">       [    0,     0,     0, ...,     5,  1045,   890],</span><br><span class="line">       [    0,     0,     0, ..., 13748, 38750,   154]])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">padded_train_sequences.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1417523, 35)</span><br></pre></td></tr></table></figure>
<p>现在数据已准备好送到RNN。</p>
<p>以下是我将使用的架构的一些元素：</p>
<ul><li>嵌入维度为300.这意味着我们将使用的80000中的每个单词都被映射到300维密集向量（浮点数）。映射将在整个训练期间进行调整。</li>
	<li>在嵌入层上应用空间丢失以减少过度拟合：它基本上查看35x300矩阵的批次并在每个矩阵中随机丢弃（设置为0）字向量（即行）。这有助于不专注于特定的单词以试图概括。</li>
	<li>双向门控循环单元（GRU）：这是循环网络部分。它是LSTM架构的更快变体。可以把它想象成两个循环网络的组合，它们在两个方向上扫描文本序列：从左到右，从右到左。这允许网络在读取给定单词时通过使用来自过去和未来信息的上下文来理解它。 GRU将多个单元作为参数，该单元是每个网络块的输出h_t的维度。我们将此数字设置为100.由于我们使用的是GRU的双向版本，因此每个RNN块的最终输出将为200。</li>
</ul><p> 双向GRU的输出具有维度（batch_size，timesteps，units）。 这意味着如果我们使用256的典型批量大小，此维度将为（256,35,200）</p>
<ul><li>在每个批次的顶部，我们应用全局平均池化，其中包括平均对应于每个时间步的输出向量（即单词）</li>
	<li>我们对最大池化应用相同的操作。</li>
	<li>我们连接前两个操作的输出。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def get_simple_rnn_model():</span><br><span class="line">    embedding_dim = 300</span><br><span class="line">    embedding_matrix = np.random.random((MAX_NB_WORDS, embedding_dim))</span><br><span class="line">    </span><br><span class="line">    inp = Input(shape=(MAX_LENGTH, ))</span><br><span class="line">    x = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, </span><br><span class="line">                  weights=[embedding_matrix], trainable=True)(inp)</span><br><span class="line">    x = SpatialDropout1D(0.3)(x)</span><br><span class="line">    x = Bidirectional(GRU(100, return_sequences=True))(x)</span><br><span class="line">    avg_pool = GlobalAveragePooling1D()(x)</span><br><span class="line">    max_pool = GlobalMaxPooling1D()(x)</span><br><span class="line">    conc = concatenate([avg_pool, max_pool])</span><br><span class="line">    outp = Dense(1, activation=&quot;sigmoid&quot;)(conc)</span><br><span class="line">    </span><br><span class="line">    model = Model(inputs=inp, outputs=outp)</span><br><span class="line">    model.compile(loss=&#x27;binary_crossentropy&#x27;,</span><br><span class="line">                  optimizer=&#x27;adam&#x27;,</span><br><span class="line">                  metrics=[&#x27;accuracy&#x27;])</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">rnn_simple_model = get_simple_rnn_model()</span><br></pre></td></tr></table></figure>
<p>让我们看看这个模型的不同层：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plot_model(rnn_simple_model, </span><br><span class="line">           to_file=&#x27;./images/article_5/rnn_simple_model.png&#x27;, </span><br><span class="line">           show_shapes=True, </span><br><span class="line">           show_layer_names=True)</span><br></pre></td></tr></table></figure>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/rnn_simple_model.png" /></p>
<p>在训练期间，使用模型检查点。 它允许在每个epoch结束时自动保存（在磁盘上）最佳模型（w.r.t精度测量）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">filepath=&quot;./models/rnn_no_embeddings/weights-improvement-&#123;epoch:02d&#125;-&#123;val_acc:.4f&#125;.hdf5&quot;</span><br><span class="line">checkpoint = ModelCheckpoint(filepath, monitor=&#x27;val_acc&#x27;, verbose=1, save_best_only=True, mode=&#x27;max&#x27;)</span><br><span class="line"></span><br><span class="line">batch_size = 256</span><br><span class="line">epochs = 2</span><br><span class="line"></span><br><span class="line">history = rnn_simple_model.fit(x=padded_train_sequences, </span><br><span class="line">                    y=y_train, </span><br><span class="line">                    validation_data=(padded_test_sequences, y_test), </span><br><span class="line">                    batch_size=batch_size, </span><br><span class="line">                    callbacks=[checkpoint], </span><br><span class="line">                    epochs=epochs, </span><br><span class="line">                    verbose=1)</span><br><span class="line"></span><br><span class="line">best_rnn_simple_model = load_model(&#x27;./models/rnn_no_embeddings/weights-improvement-01-0.8262.hdf5&#x27;)</span><br><span class="line"></span><br><span class="line">y_pred_rnn_simple = best_rnn_simple_model.predict(padded_test_sequences, verbose=1, batch_size=2048)</span><br><span class="line"></span><br><span class="line">y_pred_rnn_simple = pd.DataFrame(y_pred_rnn_simple, columns=[&#x27;prediction&#x27;])</span><br><span class="line">y_pred_rnn_simple[&#x27;prediction&#x27;] = y_pred_rnn_simple[&#x27;prediction&#x27;].map(lambda p: 1 if p &gt;= 0.5 else 0)</span><br><span class="line">y_pred_rnn_simple.to_csv(&#x27;./predictions/y_pred_rnn_simple.csv&#x27;, index=False)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred_rnn_simple = pd.read_csv(&#x27;./predictions/y_pred_rnn_simple.csv&#x27;)</span><br><span class="line">print(accuracy_score(y_test, y_pred_rnn_simple))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.826219183127</span><br></pre></td></tr></table></figure>
<p>准确率为82.6％！ 还不错！ 我们现在比以前的词袋模型表现更好，因为我们考虑了文本的顺序性质。</p>
<p>我们可以做得更好吗？</p>
<h1 id="5---Recurrent-Neural-Network-with-GloVe-pre-trained-embeddings">5 - Recurrent Neural Network with GloVe pre-trained embeddings</h1>
<p>在最后一个模型中，嵌入矩阵是随机初始化的。 如果我们可以使用预先训练的单词嵌入来初始化它怎么办？</p>
<p>让我们举一个例子：假设你的语料库中有一个单词pizza。 按照以前的架构，您可以将其初始化为随机浮点值的300维向量。 这很好。 你可以做到这一点，这种嵌入将调整整个训练过程中的进化。 但是，你可以做的而不是随机选择pizza的矢量是使用这个词的嵌入，这个词是从一个非常大的语料库中的另一个模型中学到的。 这是一种特殊的转移学习。</p>
<p>使用外部嵌入的知识可以提高RNN的精确度，因为它集成了关于单词的新信息（词汇和语义），这些信息已经在非常大的数据集上进行了训练和提炼。</p>
<p>我们将使用的预训练嵌入是<a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">GloVe</a>。</p>
<p>官方文档：GloVe是一种无监督学习算法，用于获取单词的向量表示。 对来自语料库的聚合全局词 - 词共现统计进行训练，并且所得到的表示展示词向量空间的有趣线性子结构。</p>
<p>我将使用的GloVe嵌入式的训练是在一个非常大的常见互联网爬虫中进行的，其中包括：</p>
<ul><li>840 Billion tokens,</li>
	<li>2.2 million size vocab</li>
</ul><p>压缩文件是2.03 GB下载。 请注意，此文件无法轻松加载到标准笔记本电脑上。</p>
<p>GloVe嵌入的维度是300。</p>
<p>GloVe嵌入有原始文本数据，每行包含一个单词和300个浮点数（相应的嵌入）。 所以要做的第一件事就是将这个结构转换为python字典。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def get_coefs(word, *arr):</span><br><span class="line">    try:</span><br><span class="line">        return word, np.asarray(arr, dtype=&#x27;float32&#x27;)</span><br><span class="line">    except:</span><br><span class="line">        return None, None</span><br><span class="line">    </span><br><span class="line">embeddings_index = dict(get_coefs(*o.strip().split()) for o in tqdm_notebook(open(&#x27;./embeddings/glove.840B.300d.txt&#x27;)))</span><br><span class="line"></span><br><span class="line">embed_size=300</span><br><span class="line">for k in tqdm_notebook(list(embeddings_index.keys())):</span><br><span class="line">    v = embeddings_index[k]</span><br><span class="line">    try:</span><br><span class="line">        if v.shape != (embed_size, ):</span><br><span class="line">            embeddings_index.pop(k)</span><br><span class="line">    except:</span><br><span class="line">        pass</span><br><span class="line">            </span><br><span class="line">embeddings_index.pop(None)</span><br></pre></td></tr></table></figure>
<p> 一旦创建了嵌入索引，我们提取所有向量，我们将它们堆叠在一起并计算它们的均值和标准差。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">values = list(embeddings_index.values())</span><br><span class="line">all_embs = np.stack(values)</span><br><span class="line"></span><br><span class="line">emb_mean, emb_std = all_embs.mean(), all_embs.std()</span><br></pre></td></tr></table></figure>
<p>现在我们生成嵌入矩阵。 我们将按照mean = emb_mean和std = emb_std的正态分布对其进行初始化。</p>
<p>然后我们浏览了我们语料库的80000个单词。 对于每个单词，如果它包含在GloVe中，我们选择它的嵌入。</p>
<p>否则，我们pass。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">word_index = tokenizer.word_index</span><br><span class="line">nb_words = MAX_NB_WORDS</span><br><span class="line">embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))</span><br><span class="line"></span><br><span class="line">oov = 0</span><br><span class="line">for word, i in tqdm_notebook(word_index.items()):</span><br><span class="line">    if i &gt;= MAX_NB_WORDS: continue</span><br><span class="line">    embedding_vector = embeddings_index.get(word)</span><br><span class="line">    if embedding_vector is not None:</span><br><span class="line">        embedding_matrix[i] = embedding_vector</span><br><span class="line">    else:</span><br><span class="line">        oov += 1</span><br><span class="line"></span><br><span class="line">print(oov)</span><br><span class="line"></span><br><span class="line">def get_rnn_model_with_glove_embeddings():</span><br><span class="line">    embedding_dim = 300</span><br><span class="line">    inp = Input(shape=(MAX_LENGTH, ))</span><br><span class="line">    x = Embedding(MAX_NB_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=True)(inp)</span><br><span class="line">    x = SpatialDropout1D(0.3)(x)</span><br><span class="line">    x = Bidirectional(GRU(100, return_sequences=True))(x)</span><br><span class="line">    avg_pool = GlobalAveragePooling1D()(x)</span><br><span class="line">    max_pool = GlobalMaxPooling1D()(x)</span><br><span class="line">    conc = concatenate([avg_pool, max_pool])</span><br><span class="line">    outp = Dense(1, activation=&quot;sigmoid&quot;)(conc)</span><br><span class="line">    </span><br><span class="line">    model = Model(inputs=inp, outputs=outp)</span><br><span class="line">    model.compile(loss=&#x27;binary_crossentropy&#x27;,</span><br><span class="line">                  optimizer=&#x27;adam&#x27;,</span><br><span class="line">                  metrics=[&#x27;accuracy&#x27;])</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">rnn_model_with_embeddings = get_rnn_model_with_glove_embeddings()</span><br><span class="line"></span><br><span class="line">filepath=&quot;./models/rnn_with_embeddings/weights-improvement-&#123;epoch:02d&#125;-&#123;val_acc:.4f&#125;.hdf5&quot;</span><br><span class="line">checkpoint = ModelCheckpoint(filepath, monitor=&#x27;val_acc&#x27;, verbose=1, save_best_only=True, mode=&#x27;max&#x27;)</span><br><span class="line"></span><br><span class="line">batch_size = 256</span><br><span class="line">epochs = 4</span><br><span class="line"></span><br><span class="line">history = rnn_model_with_embeddings.fit(x=padded_train_sequences, </span><br><span class="line">                    y=y_train, </span><br><span class="line">                    validation_data=(padded_test_sequences, y_test), </span><br><span class="line">                    batch_size=batch_size, </span><br><span class="line">                    callbacks=[checkpoint], </span><br><span class="line">                    epochs=epochs, </span><br><span class="line">                    verbose=1)</span><br><span class="line"></span><br><span class="line">best_rnn_model_with_glove_embeddings = load_model(&#x27;./models/rnn_with_embeddings/weights-improvement-03-0.8372.hdf5&#x27;)</span><br><span class="line"></span><br><span class="line">y_pred_rnn_with_glove_embeddings = best_rnn_model_with_glove_embeddings.predict(</span><br><span class="line">    padded_test_sequences, verbose=1, batch_size=2048)</span><br><span class="line"></span><br><span class="line">y_pred_rnn_with_glove_embeddings = pd.DataFrame(y_pred_rnn_with_glove_embeddings, columns=[&#x27;prediction&#x27;])</span><br><span class="line">y_pred_rnn_with_glove_embeddings[&#x27;prediction&#x27;] = y_pred_rnn_with_glove_embeddings[&#x27;prediction&#x27;].map(lambda p: </span><br><span class="line">                                                                                                    1 if p &gt;= 0.5 else 0)</span><br><span class="line">y_pred_rnn_with_glove_embeddings.to_csv(&#x27;./predictions/y_pred_rnn_with_glove_embeddings.csv&#x27;, index=False)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred_rnn_with_glove_embeddings = pd.read_csv(&#x27;./predictions/y_pred_rnn_with_glove_embeddings.csv&#x27;)</span><br><span class="line">print(accuracy_score(y_test, y_pred_rnn_with_glove_embeddings))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.837203100893</span><br></pre></td></tr></table></figure>
<p>精度为83.7％！ 从外部词嵌入转移学习有效！ 对于本教程的其余部分，我将在嵌入矩阵中使用GloVe嵌入。</p>
<h1 id="6---Multi-channel-Convolutional-Neural-Network">6 - Multi-channel Convolutional Neural Network</h1>
<p>在本节中，我正在尝试我在<a target="_blank" rel="noopener" href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">这里</a>阅读的卷积神经网络架构。 CNN通常用于计算机视觉。 但是，我们最近开始将它们应用于NLP任务，结果很有希望。</p>
<p>让我们简要地看一下当我们在文本数据上使用卷积时会发生什么。 为了解释这一点，我从wildm.com（一个非常好的博客）借用这个着名的图表（下面）！</p>
<p>让我们考虑它使用的例子：我非常喜欢这部电影！ （7个代币）</p>
<ul><li>每个单词的嵌入维度为5.因此，该句子由维度矩阵（7,5）表示。 您可以将其视为“图像”（〜数字/浮点矩阵）。</li>
	<li>6个过滤器，2个尺寸（2,5）（3,5）和（4,5）应用于该矩阵。 这些滤波器的特殊性在于它们不是方形矩阵，其宽度等于嵌入矩阵的宽度。 因此每个卷积的结果将是列向量。</li>
	<li>使用最大池化操作对从卷积得到的每个列向量进行二次采样。</li>
	<li>最大池化操作的结果在最终向量中连接，该向量被传递给softmax函数以进行分类。</li>
</ul><p>背后的直觉是什么？</p>
<p>当检测到特殊模式时，每个卷积的结果将触发。 通过改变内核的大小并连接它们的输出，您可以自己检测多个大小的模式（2,3或5个相邻的单词）。</p>
<p>模式可以是表达式（单词ngrams？），如“我讨厌”，“非常好”，因此CNN可以在句子中识别它们而不管它们的位置如何。</p>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/cnn_text.png" /></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">def get_cnn_model():</span><br><span class="line">    embedding_dim = 300</span><br><span class="line">    </span><br><span class="line">    filter_sizes = [2, 3, 5]</span><br><span class="line">    num_filters = 256</span><br><span class="line">    drop = 0.3</span><br><span class="line"></span><br><span class="line">    inputs = Input(shape=(MAX_LENGTH,), dtype=&#x27;int32&#x27;)</span><br><span class="line">    embedding = Embedding(input_dim=MAX_NB_WORDS,</span><br><span class="line">                                output_dim=embedding_dim,</span><br><span class="line">                                weights=[embedding_matrix],</span><br><span class="line">                                input_length=MAX_LENGTH,</span><br><span class="line">                                trainable=True)(inputs)</span><br><span class="line"></span><br><span class="line">    reshape = Reshape((MAX_LENGTH, embedding_dim, 1))(embedding)</span><br><span class="line">    conv_0 = Conv2D(num_filters, </span><br><span class="line">                    kernel_size=(filter_sizes[0], embedding_dim), </span><br><span class="line">                    padding=&#x27;valid&#x27;, kernel_initializer=&#x27;normal&#x27;, </span><br><span class="line">                    activation=&#x27;relu&#x27;)(reshape)</span><br><span class="line"></span><br><span class="line">    conv_1 = Conv2D(num_filters, </span><br><span class="line">                    kernel_size=(filter_sizes[1], embedding_dim), </span><br><span class="line">                    padding=&#x27;valid&#x27;, kernel_initializer=&#x27;normal&#x27;, </span><br><span class="line">                    activation=&#x27;relu&#x27;)(reshape)</span><br><span class="line">    conv_2 = Conv2D(num_filters, </span><br><span class="line">                    kernel_size=(filter_sizes[2], embedding_dim), </span><br><span class="line">                    padding=&#x27;valid&#x27;, kernel_initializer=&#x27;normal&#x27;, </span><br><span class="line">                    activation=&#x27;relu&#x27;)(reshape)</span><br><span class="line"></span><br><span class="line">    maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), </span><br><span class="line">                          strides=(1,1), padding=&#x27;valid&#x27;)(conv_0)</span><br><span class="line"></span><br><span class="line">    maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), </span><br><span class="line">                          strides=(1,1), padding=&#x27;valid&#x27;)(conv_1)</span><br><span class="line"></span><br><span class="line">    maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), </span><br><span class="line">                          strides=(1,1), padding=&#x27;valid&#x27;)(conv_2)</span><br><span class="line">    concatenated_tensor = Concatenate(axis=1)(</span><br><span class="line">        [maxpool_0, maxpool_1, maxpool_2])</span><br><span class="line">    flatten = Flatten()(concatenated_tensor)</span><br><span class="line">    dropout = Dropout(drop)(flatten)</span><br><span class="line">    output = Dense(units=1, activation=&#x27;sigmoid&#x27;)(dropout)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=inputs, outputs=output)</span><br><span class="line">    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=adam, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])</span><br><span class="line">    </span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">cnn_model_multi_channel = get_cnn_model()</span><br><span class="line"></span><br><span class="line">plot_model(cnn_model_multi_channel, </span><br><span class="line">           to_file=&#x27;./images/article_5/cnn_model_multi_channel.png&#x27;, </span><br><span class="line">           show_shapes=True, </span><br><span class="line">           show_layer_names=True)</span><br></pre></td></tr></table></figure>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/cnn_model_multi_channel.png" /></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">filepath=&quot;./models/cnn_multi_channel/weights-improvement-&#123;epoch:02d&#125;-&#123;val_acc:.4f&#125;.hdf5&quot;</span><br><span class="line">checkpoint = ModelCheckpoint(filepath, monitor=&#x27;val_acc&#x27;, verbose=1, save_best_only=True, mode=&#x27;max&#x27;)</span><br><span class="line"></span><br><span class="line">batch_size = 256</span><br><span class="line">epochs = 4</span><br><span class="line"></span><br><span class="line">history = cnn_model_multi_channel.fit(x=padded_train_sequences, </span><br><span class="line">                    y=y_train, </span><br><span class="line">                    validation_data=(padded_test_sequences, y_test), </span><br><span class="line">                    batch_size=batch_size, </span><br><span class="line">                    callbacks=[checkpoint], </span><br><span class="line">                    epochs=epochs, </span><br><span class="line">                    verbose=1)</span><br><span class="line"></span><br><span class="line">best_cnn_model = load_model(&#x27;./models/cnn_multi_channel/weights-improvement-04-0.8264.hdf5&#x27;)</span><br><span class="line"></span><br><span class="line">y_pred_cnn_multi_channel = best_cnn_model.predict(padded_test_sequences, verbose=1, batch_size=2048)</span><br><span class="line"></span><br><span class="line">y_pred_cnn_multi_channel = pd.DataFrame(y_pred_cnn_multi_channel, columns=[&#x27;prediction&#x27;])</span><br><span class="line">y_pred_cnn_multi_channel[&#x27;prediction&#x27;] = y_pred_cnn_multi_channel[&#x27;prediction&#x27;].map(lambda p: 1 if p &gt;= 0.5 else 0)</span><br><span class="line">y_pred_cnn_multi_channel.to_csv(&#x27;./predictions/y_pred_cnn_multi_channel.csv&#x27;, index=False)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred_cnn_multi_channel = pd.read_csv(&#x27;./predictions/y_pred_cnn_multi_channel.csv&#x27;)</span><br><span class="line">print(accuracy_score(y_test, y_pred_cnn_multi_channel))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.826409655689</span><br></pre></td></tr></table></figure>
<p>82.6％的准确度，我们不如RNN精确，但仍然优于BOW模型。 也许对超参数（过滤器的数量和大小）的调查给出了优势？</p>
<h1 id="7---Recurrent-+-Convolutional-neural-network">7 - Recurrent + Convolutional neural network</h1>
<p>RNN功能强大。 但是，有些人发现通过在回流层顶部添加卷积层可以使它们更加稳健。</p>
<p>理性的背后是RNN允许您嵌入有关序列和先前单词的信息，CNN采用此嵌入并从中提取局部特征。 将这两个层一起工作是一个成功的组合。</p>
<p>更多关于<a target="_blank" rel="noopener" href="http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/">这里</a>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def get_rnn_cnn_model():</span><br><span class="line">    embedding_dim = 300</span><br><span class="line">    inp = Input(shape=(MAX_LENGTH, ))</span><br><span class="line">    x = Embedding(MAX_NB_WORDS, embedding_dim, weights=[ embedding_matrix], input_length=MAX_LENGTH, trainable=True)(inp)</span><br><span class="line">    x = SpatialDropout1D(0.3)(x)</span><br><span class="line">    x = Bidirectional(GRU(100, return_sequences=True))(x)</span><br><span class="line">    x = Conv1D(64, kernel_size = 2, padding = &quot;valid&quot;, kernel_initializer = &quot;he_uniform&quot;)(x)</span><br><span class="line">    avg_pool = GlobalAveragePooling1D()(x)</span><br><span class="line">    max_pool = GlobalMaxPooling1D()(x)</span><br><span class="line">    conc = concatenate([avg_pool, max_pool])</span><br><span class="line">    outp = Dense(1, activation=&quot;sigmoid&quot;)(conc)</span><br><span class="line">    </span><br><span class="line">    model = Model(inputs=inp, outputs=outp)</span><br><span class="line">    model.compile(loss=&#x27;binary_crossentropy&#x27;,</span><br><span class="line">                  optimizer=&#x27;adam&#x27;,</span><br><span class="line">                  metrics=[&#x27;accuracy&#x27;])</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">rnn_cnn_model = get_rnn_cnn_model()</span><br><span class="line"></span><br><span class="line">plot_model(rnn_cnn_model, to_file=&#x27;./images/article_5/rnn_cnn_model.png&#x27;, show_shapes=True, show_layer_names=True)</span><br></pre></td></tr></table></figure>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/rnn_cnn_model.png" /></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">filepath=&quot;./models/rnn_cnn/weights-improvement-&#123;epoch:02d&#125;-&#123;val_acc:.4f&#125;.hdf5&quot;</span><br><span class="line">checkpoint = ModelCheckpoint(filepath, monitor=&#x27;val_acc&#x27;, verbose=1, save_best_only=True, mode=&#x27;max&#x27;)</span><br><span class="line"></span><br><span class="line">batch_size = 256</span><br><span class="line">epochs = 4</span><br><span class="line"></span><br><span class="line">history = rnn_cnn_model.fit(x=padded_train_sequences, </span><br><span class="line">                    y=y_train, </span><br><span class="line">                    validation_data=(padded_test_sequences, y_test), </span><br><span class="line">                    batch_size=batch_size, </span><br><span class="line">                    callbacks=[checkpoint], </span><br><span class="line">                    epochs=epochs, </span><br><span class="line">                    verbose=1)</span><br><span class="line"></span><br><span class="line">best_rnn_cnn_model = load_model(&#x27;./models/rnn_cnn/weights-improvement-03-0.8379.hdf5&#x27;)</span><br><span class="line"></span><br><span class="line">y_pred_rnn_cnn = best_rnn_cnn_model.predict(padded_test_sequences, verbose=1, batch_size=2048)</span><br><span class="line"></span><br><span class="line">y_pred_rnn_cnn = pd.DataFrame(y_pred_rnn_cnn, columns=[&#x27;prediction&#x27;])</span><br><span class="line">y_pred_rnn_cnn[&#x27;prediction&#x27;] = y_pred_rnn_cnn[&#x27;prediction&#x27;].map(lambda p: 1 if p &gt;= 0.5 else 0)</span><br><span class="line">y_pred_rnn_cnn.to_csv(&#x27;./predictions/y_pred_rnn_cnn.csv&#x27;, index=False)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred_rnn_cnn = pd.read_csv(&#x27;./predictions/y_pred_rnn_cnn.csv&#x27;)</span><br><span class="line">print(accuracy_score(y_test, y_pred_rnn_cnn))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.837882453033</span><br></pre></td></tr></table></figure>
<p>准确率为83.8％。 迄今为止最好的模特。</p>
<h1 id="8----Conclusion">8 - Conclusion</h1>
<p>我们运行了七种不同的模型。 让我们看看他们如何比较：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">sns.set_style(&quot;whitegrid&quot;)</span><br><span class="line">sns.set_palette(&quot;pastel&quot;)</span><br><span class="line"></span><br><span class="line">predictions_files = os.listdir(&#x27;./predictions/&#x27;)</span><br><span class="line"></span><br><span class="line">predictions_dfs = []</span><br><span class="line">for f in predictions_files:</span><br><span class="line">    aux = pd.read_csv(&#x27;./predictions/&#123;0&#125;&#x27;.format(f))</span><br><span class="line">    aux.columns = [f.strip(&#x27;.csv&#x27;)]</span><br><span class="line">    predictions_dfs.append(aux)</span><br><span class="line"></span><br><span class="line">predictions = pd.concat(predictions_dfs, axis=1)</span><br><span class="line"></span><br><span class="line">scores = &#123;&#125;</span><br><span class="line"></span><br><span class="line">for column in tqdm_notebook(predictions.columns, leave=False):</span><br><span class="line">    if column != &#x27;y_true&#x27;:</span><br><span class="line">        s = accuracy_score(predictions[&#x27;y_true&#x27;].values, predictions[column].values)</span><br><span class="line">        scores[column] = s</span><br><span class="line"></span><br><span class="line">scores = pd.DataFrame([scores], index=[&#x27;accuracy&#x27;])</span><br><span class="line"></span><br><span class="line">mapping_name = dict(zip(list(scores.columns), </span><br><span class="line">                        [&#x27;Char ngram + LR&#x27;, &#x27;(Word + Char ngram) + LR&#x27;, </span><br><span class="line">                           &#x27;Word ngram + LR&#x27;, &#x27;CNN (multi channel)&#x27;,</span><br><span class="line">                           &#x27;RNN + CNN&#x27;, &#x27;RNN no embd.&#x27;, &#x27;RNN + GloVe embds.&#x27;]))</span><br><span class="line"></span><br><span class="line">scores = scores.rename(columns=mapping_name)</span><br><span class="line">scores = scores[[&#x27;Word ngram + LR&#x27;, &#x27;Char ngram + LR&#x27;, &#x27;(Word + Char ngram) + LR&#x27;,</span><br><span class="line">                &#x27;RNN no embd.&#x27;, &#x27;RNN + GloVe embds.&#x27;, &#x27;CNN (multi channel)&#x27;,</span><br><span class="line">                &#x27;RNN + CNN&#x27;]]</span><br><span class="line"></span><br><span class="line">scores = scores.T</span><br><span class="line"></span><br><span class="line">ax = scores[&#x27;accuracy&#x27;].plot(kind=&#x27;bar&#x27;, </span><br><span class="line">                             figsize=(16, 5), </span><br><span class="line">                             ylim=(scores.accuracy.min()*0.97, scores.accuracy.max() * 1.01), </span><br><span class="line">                             color=&#x27;red&#x27;, </span><br><span class="line">                             alpha=0.75, </span><br><span class="line">                             rot=45, </span><br><span class="line">                             fontsize=13)</span><br><span class="line">ax.set_title(&#x27;Comparative accuracy of the different models&#x27;)</span><br><span class="line"></span><br><span class="line">for i in ax.patches:</span><br><span class="line">    ax.annotate(str(round(i.get_height(), 3)), </span><br><span class="line">                (i.get_x() + 0.1, i.get_height() * 1.002), color=&#x27;dimgrey&#x27;, fontsize=14)</span><br></pre></td></tr></table></figure>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/benchmark.png" /></p>
<p>让我们快速检查模型预测之间的相关性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(10, 5))</span><br><span class="line">sns.heatmap(predictions.drop(&#x27;y_true&#x27;, axis=1).corr(method=&#x27;kendall&#x27;), cmap=&quot;Blues&quot;, annot=True);</span><br></pre></td></tr></table></figure>
<p style="text-align:center;"><img alt="" class="has" src="https://ahmedbesbes.com/images/article_5/heatmap.png" /></p>
<h1 id="Conclusion">Conclusion</h1>
<p>以下是我认为值得分享的快速发现：</p>
<ul><li>使用字符ngram的词袋模型可以非常有效。 不要低估他们！ 它们的计算成本相对较低，而且易于理解。</li>
	<li>RNN功能强大。 但是，您有时可以使用GloVe等外部预先训练的嵌入物来泵送它们。 您还可以使用其他流行的嵌入，如word2vec和FastText。</li>
	<li>CNN可以应用于文本。 他们的主要优势是训练速度非常快。 此外，它们从文本中提取局部特征的能力对于nlp任务特别有意义。</li>
	<li>RNN和CNN可以堆叠在一起，以利用两种架构的优势。</li>
</ul><p>这篇文章很长，我希望你喜欢它。 如果您有任何问题或建议，请随时发表评论。</p>
<h1 id="Some-helpful-links-to-explore">Some helpful links to explore</h1>
<p>这是我在撰写这篇文章时使用的很好的资源：</p>
<ul><li><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
	<li><a target="_blank" rel="noopener" href="http://wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">http://wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</a></li>
</ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://qubeijun.github.io">qubeijun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://qubeijun.github.io/posts/11019.html">https://qubeijun.github.io/posts/11019.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://qubeijun.github.io" target="_blank">qubeijun的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/cnn/">cnn</a><a class="post-meta__tags" href="/tags/%E5%88%86%E7%B1%BB/">分类</a><a class="post-meta__tags" href="/tags/rnn/">rnn</a><a class="post-meta__tags" href="/tags/GloVe/">GloVe</a></div><div class="post-share"><div class="social-share" data-image="/image/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/image/wechatpay.jpg" target="_blank"><img class="post-qr-code-img" src="/image/wechatpay.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/image/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/image/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/posts/42971.html" title="（翻译）Understanding Convolutional Neural Networks for NLP"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">（翻译）Understanding Convolutional Neural Networks for NLP</div></div></a><a class="next-post pull-right" href="/posts/64667.html" title="小木虫论坛-学术科研互动平台爬虫"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">小木虫论坛-学术科研互动平台爬虫</div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a href="/posts/42971.html" title="（翻译）Understanding Convolutional Neural Networks for NLP"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-04-15</div><div class="title">（翻译）Understanding Convolutional Neural Networks for NLP</div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/image/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qubeijun</div><div class="author-info-description">qubeijun的博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">78</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qubeijun"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qubeijun" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="/18640274769@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a><a class="social-icon" href="/qu825954789" target="_blank" title=""><i class="fa-brands fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">喜欢瞎折腾的一个人 <div style='text-align:center'><img height='150' width='150' src="/image/wechat.jpg"/><img height='150' width='150' src="/image/douyin.jpg"/><img height='150' width='150' src="/image/beikebiaoqing.png"/></div></div></div><div class="card-widget"><div class="item-headline"><i class="fa-solid fa-calendar"></i><span>纪念日</span></div><div class="item-content">和她❤️恋爱已经&nbsp;<timing1></timing1><br> 和她❤️领证已经&nbsp;<timing2></timing2><br> 和她❤️订婚已经&nbsp;<timing3></timing3><br> 和她❤️结婚已经&nbsp;<timing4></timing4><br> 和她❤️生娃已经&nbsp;<timing5></timing5></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0---Data-pre-processing"><span class="toc-text">0 - Data pre-processing</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1---Bag-of-word-model-based-on-word-ngrams"><span class="toc-text">1 - Bag of word model based on word ngrams</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2---Bag-of-word-model-based-on-character-ngrams"><span class="toc-text">2 - Bag of word model based on character ngrams</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3---Bag-of-word-model-based-on-word-and-character-ngrams"><span class="toc-text">3 - Bag of word model based on word and character ngrams</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4---Recurrent-Neural-Network-without-pre-trained-embedding"><span class="toc-text">4 - Recurrent Neural Network without pre-trained embedding</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5---Recurrent-Neural-Network-with-GloVe-pre-trained-embeddings"><span class="toc-text">5 - Recurrent Neural Network with GloVe pre-trained embeddings</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6---Multi-channel-Convolutional-Neural-Network"><span class="toc-text">6 - Multi-channel Convolutional Neural Network</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7---Recurrent-+-Convolutional-neural-network"><span class="toc-text">7 - Recurrent + Convolutional neural network</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8----Conclusion"><span class="toc-text">8 - Conclusion</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Conclusion"><span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Some-helpful-links-to-explore"><span class="toc-text">Some helpful links to explore</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2024 By qubeijun</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://qubeijun.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'XYmqxeHh8UwYifOg99QJM6Ao-gzGzoHsz',
      appKey: 'CJlknwAcFvxc6RBreHAWkxEz',
      avatar: 'monsterid',
      serverURLs: 'https://xymqxehh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await btf.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script src="/html/vue.js"></script><script src="/html/timing.js"><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>